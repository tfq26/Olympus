// server.js
import dotenv from "dotenv";
dotenv.config();

// ‚úÖ Import the correct MCP server class
import { McpServer } from "@modelcontextprotocol/sdk/server/mcp.js";

import express from "express";
import { WebSocketServer } from "ws";
import { interpretMessage, queryAI } from "./model/router.js";

// 1Ô∏è‚É£ Define MCP Tools
const tools = {
  echo: {
    description: "Echoes text back to the user",
    run: async ({ text }) => `Echo: ${text}`,
  },
  getLogs: {
    description: "Fetches logs",
    run: async ({ status }) => `Logs fetched with status: ${status}`,
  },
  getResource: {
    description: "Retrieves resource info",
    run: async ({ id }) => `Resource ${id}: { uptime: '24h', usage: '80%' }`,
  },
  queryAI: {
    description: "Query the NVIDIA AI model with any prompt",
    run: async ({ prompt, systemMessage }) => {
      try {
        const response = await queryAI(prompt, systemMessage);
        return response;
      } catch (error) {
        return `Error querying AI: ${error.message}`;
      }
    },
  },
};

// 2Ô∏è‚É£ Create MCP Server
const server = new McpServer({ tools });

// 3Ô∏è‚É£ Express + WebSocket setup
const app = express();
const wss = new WebSocketServer({ noServer: true });
const PORT = 8080;

app.get("/", (req, res) => {
  res.send("‚úÖ MCP Server is running on ws://localhost:" + PORT);
});

const httpServer = app.listen(PORT, () =>
  console.log(`üöÄ MCP Server ready on ws://localhost:${PORT}`)
);

// 4Ô∏è‚É£ WebSocket Message Handling
httpServer.on("upgrade", (req, socket, head) => {
  wss.handleUpgrade(req, socket, head, (ws) => {
    ws.on("message", async (data) => {
      try {
        const payload = JSON.parse(data.toString());
        
        // Check if direct tool/args are provided (bypasses router)
        if (payload.tool && payload.args) {
          console.log("üîß Direct tool call:", payload.tool, payload.args);
          
          if (!tools[payload.tool]) {
            throw new Error(`Unknown tool: ${payload.tool}`);
          }

          const result = await tools[payload.tool].run(payload.args);
          console.log("‚úÖ Tool result:", result);

          // Send result back to client
          ws.send(JSON.stringify({ reply: result }));
        } 
        // Otherwise, use message routing through NVIDIA model
        else if (payload.message) {
          console.log("üó£Ô∏è User:", payload.message);

          // Ask your NVIDIA model how to route it
          const { tool, args } = await interpretMessage(payload.message);
          console.log("üß© Routed to:", tool, args);

          // Run the mapped tool
          if (!tools[tool]) {
            throw new Error(`Unknown tool: ${tool}`);
          }

          const result = await tools[tool].run(args);
          console.log("‚úÖ Tool result:", result);

          // Send result back to client
          ws.send(JSON.stringify({ reply: result }));
        } else {
          throw new Error("Invalid payload: must include either 'message' or 'tool' and 'args'");
        }
      } catch (err) {
        console.error("‚ùå Error processing message:", err);
        ws.send(JSON.stringify({ error: err.message }));
      }
    });
  });
});
